{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6897944,"sourceType":"datasetVersion","datasetId":3962399},{"sourceId":213216,"sourceType":"datasetVersion","datasetId":91827}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Standard libraries\nimport re\nimport os\nimport time\nimport string\nfrom pathlib import Path\nfrom collections import Counter\n\n# Data Science\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Scikit-learn\n\n# Data preparation and preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Models\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Analysis\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T00:19:33.930458Z","iopub.execute_input":"2025-08-03T00:19:33.930736Z","iopub.status.idle":"2025-08-03T00:19:33.936707Z","shell.execute_reply.started":"2025-08-03T00:19:33.930716Z","shell.execute_reply":"2025-08-03T00:19:33.935781Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"ROOT_DIR = Path(\"/kaggle/input/email-spam-classification-dataset\")\nDATA_DIR = ROOT_DIR / \"combined_data.csv\"\nmaster_df = pd.read_csv(DATA_DIR)\nX, y = master_df['text'], master_df['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T00:19:30.018071Z","iopub.execute_input":"2025-08-03T00:19:30.018677Z","iopub.status.idle":"2025-08-03T00:19:33.875208Z","shell.execute_reply.started":"2025-08-03T00:19:30.018650Z","shell.execute_reply":"2025-08-03T00:19:33.874353Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2,      # 80% train, 20% test\n    stratify=y,         # KEY: Preserve class distribution\n    random_state=42     # Reproducibility\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T00:19:33.876004Z","iopub.execute_input":"2025-08-03T00:19:33.876311Z","iopub.status.idle":"2025-08-03T00:19:33.926731Z","shell.execute_reply.started":"2025-08-03T00:19:33.876286Z","shell.execute_reply":"2025-08-03T00:19:33.925632Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# SPAM DETECTION EDA: VALIDATING OUR HYPOTHESES\n# ============================================================================\n\nprint(\"üîç SPAM DETECTION EDA: VALIDATING FEATURE HYPOTHESES\")\nprint(\"=\" * 60)\n\n# Basic dataset overview\nprint(f\"Dataset shape: {master_df.shape}\")\nprint(f\"Class distribution:\")\nprint(master_df['label'].value_counts())\nprint(f\"Spam rate: {master_df['label'].mean():.1%}\")\n\n# Separate spam and ham for analysis\nspam_emails = X_train[y_train == 1]  # Only training data\nham_emails = X_train[y_train == 0]   # Only training data\n\nprint(f\"\\nSpam emails: {len(spam_emails)}\")\nprint(f\"Ham emails: {len(ham_emails)}\")\n\n# ============================================================================\n# HYPOTHESIS 1: PUNCTUATION PATTERNS\n# ============================================================================\n\ndef analyze_punctuation_patterns(emails, label_name):\n    \"\"\"Analyze punctuation usage patterns\"\"\"\n    print(f\"\\nüìù PUNCTUATION ANALYSIS - {label_name.upper()}\")\n    print(\"-\" * 40)\n    \n    # Count various punctuation patterns\n    exclamation_counts = [email.count('!') for email in emails]\n    question_counts = [email.count('?') for email in emails]\n    caps_ratios = [sum(1 for c in email if c.isupper()) / len(email) if len(email) > 0 else 0 for email in emails]\n    \n    results = {\n        'avg_exclamations': np.mean(exclamation_counts),\n        'max_exclamations': np.max(exclamation_counts),\n        'excessive_exclamations': sum(1 for count in exclamation_counts if count > 3),\n        'avg_caps_ratio': np.mean(caps_ratios),\n        'high_caps_emails': sum(1 for ratio in caps_ratios if ratio > 0.3)\n    }\n    \n    for key, value in results.items():\n        print(f\"  {key}: {value:.3f}\")\n    \n    return results\n\n# Analyze punctuation for both classes\nspam_punct = analyze_punctuation_patterns(spam_emails, \"spam\")\nham_punct = analyze_punctuation_patterns(ham_emails, \"ham\")\n\n# ============================================================================\n# HYPOTHESIS 2: SUSPICIOUS URLS AND DOMAINS\n# ============================================================================\n\ndef analyze_url_patterns(emails, label_name):\n    \"\"\"Analyze URL patterns in emails\"\"\"\n    print(f\"\\nüåê URL ANALYSIS - {label_name.upper()}\")\n    print(\"-\" * 30)\n    \n    # URL pattern matching\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    shortened_patterns = [r'bit\\.ly', r'tinyurl', r'goo\\.gl', r't\\.co']\n    \n    urls_found = []\n    emails_with_urls = 0\n    emails_with_short_urls = 0\n    \n    for email in emails:\n        email_urls = re.findall(url_pattern, email.lower())\n        if email_urls:\n            emails_with_urls += 1\n            urls_found.extend(email_urls)\n            \n        # Check for shortened URLs\n        if any(re.search(pattern, email.lower()) for pattern in shortened_patterns):\n            emails_with_short_urls += 1\n    \n    print(f\"  Emails with URLs: {emails_with_urls} ({emails_with_urls/len(emails)*100:.1f}%)\")\n    print(f\"  Emails with shortened URLs: {emails_with_short_urls} ({emails_with_short_urls/len(emails)*100:.1f}%)\")\n    print(f\"  Total URLs found: {len(urls_found)}\")\n    \n    # Show sample URLs (first 5)\n    if urls_found:\n        print(f\"  Sample URLs: {urls_found[:5]}\")\n    \n    return {\n        'url_percentage': emails_with_urls/len(emails)*100,\n        'short_url_percentage': emails_with_short_urls/len(emails)*100,\n        'total_urls': len(urls_found)\n    }\n\nspam_urls = analyze_url_patterns(spam_emails, \"spam\")\nham_urls = analyze_url_patterns(ham_emails, \"ham\")\n\n# ============================================================================\n# HYPOTHESIS 3: CHARACTER SUBSTITUTIONS & OBFUSCATION\n# ============================================================================\n\ndef analyze_substitution_patterns(emails, label_name):\n    \"\"\"Analyze character substitution and obfuscation\"\"\"\n    print(f\"\\nüîÄ SUBSTITUTION ANALYSIS - {label_name.upper()}\")\n    print(\"-\" * 35)\n    \n    # Common substitution patterns\n    substitution_patterns = {\n        'number_subs': r'[0-9]',  # Numbers in words\n        'at_symbol': r'@',        # @ for 'a'\n        'dollar_sign': r'\\$',     # $ for 's'\n        'mixed_case': r'[a-z][A-Z]|[A-Z][a-z]'  # Mixed case within words\n    }\n    \n    results = {}\n    for pattern_name, pattern in substitution_patterns.items():\n        count = sum(1 for email in emails if re.search(pattern, email))\n        percentage = count / len(emails) * 100\n        results[pattern_name] = percentage\n        print(f\"  {pattern_name}: {count} emails ({percentage:.1f}%)\")\n    \n    # Look for specific obfuscation examples\n    obfuscation_examples = []\n    for email in emails[:100]:  # Check first 100 emails\n        words = email.split()\n        for word in words:\n            if re.search(r'[0-9@\\$]', word) and len(word) > 3:\n                obfuscation_examples.append(word)\n                if len(obfuscation_examples) >= 10:\n                    break\n        if len(obfuscation_examples) >= 10:\n            break\n    \n    if obfuscation_examples:\n        print(f\"  Example obfuscated words: {obfuscation_examples[:5]}\")\n    \n    return results\n\nspam_subs = analyze_substitution_patterns(spam_emails, \"spam\")\nham_subs = analyze_substitution_patterns(ham_emails, \"ham\")\n\n# ============================================================================\n# HYPOTHESIS 4: WORD FREQUENCY ANALYSIS\n# ============================================================================\n\ndef get_common_words(emails, label_name, top_n=20):\n    \"\"\"Get most common words in emails\"\"\"\n    print(f\"\\nüìä WORD FREQUENCY - {label_name.upper()}\")\n    print(\"-\" * 30)\n    \n    # Combine all emails and split into words\n    all_text = ' '.join(emails).lower()\n    # Remove punctuation and numbers for cleaner word analysis\n    translator = str.maketrans('', '', string.punctuation + string.digits)\n    clean_text = all_text.translate(translator)\n    words = clean_text.split()\n    \n    # Count word frequencies\n    word_counts = Counter(words)\n    common_words = word_counts.most_common(top_n)\n    \n    print(f\"  Top {top_n} words:\")\n    for i, (word, count) in enumerate(common_words, 1):\n        print(f\"    {i:2d}. {word:15} ({count:,} times)\")\n    \n    return dict(common_words)\n\nspam_words = get_common_words(spam_emails, \"spam\", 15)\nham_words = get_common_words(ham_emails, \"ham\", 15)\n\n# ============================================================================\n# HYPOTHESIS 5: EMAIL LENGTH PATTERNS\n# ============================================================================\n\ndef analyze_length_patterns(emails, label_name):\n    \"\"\"Analyze email length characteristics\"\"\"\n    print(f\"\\nüìè LENGTH ANALYSIS - {label_name.upper()}\")\n    print(\"-\" * 25)\n    \n    lengths = [len(email) for email in emails]\n    word_counts = [len(email.split()) for email in emails]\n    \n    print(f\"  Average length: {np.mean(lengths):.1f} characters\")\n    print(f\"  Median length: {np.median(lengths):.1f} characters\")\n    print(f\"  Average words: {np.mean(word_counts):.1f} words\")\n    print(f\"  Short emails (<50 chars): {sum(1 for l in lengths if l < 50)} ({sum(1 for l in lengths if l < 50)/len(lengths)*100:.1f}%)\")\n    print(f\"  Long emails (>500 chars): {sum(1 for l in lengths if l > 500)} ({sum(1 for l in lengths if l > 500)/len(lengths)*100:.1f}%)\")\n    \n    return {\n        'avg_length': np.mean(lengths),\n        'avg_words': np.mean(word_counts),\n        'short_email_pct': sum(1 for l in lengths if l < 50)/len(lengths)*100\n    }\n\nspam_length = analyze_length_patterns(spam_emails, \"spam\")\nham_length = analyze_length_patterns(ham_emails, \"ham\")\n\n# ============================================================================\n# HYPOTHESIS VALIDATION SUMMARY\n# ============================================================================\n\nprint(f\"\\nüéØ HYPOTHESIS VALIDATION SUMMARY\")\nprint(\"=\" * 50)\n\nprint(f\"‚úÖ PUNCTUATION HYPOTHESIS:\")\nprint(f\"   Spam emails use {spam_punct['avg_exclamations']:.1f}x more exclamations than ham\")\nprint(f\"   {spam_punct['excessive_exclamations']/len(spam_emails)*100:.1f}% of spam vs {ham_punct['excessive_exclamations']/len(ham_emails)*100:.1f}% of ham use excessive exclamations\")\n\nprint(f\"\\n‚úÖ URL HYPOTHESIS:\")\nprint(f\"   {spam_urls['url_percentage']:.1f}% of spam vs {ham_urls['url_percentage']:.1f}% of ham contain URLs\")\nprint(f\"   {spam_urls['short_url_percentage']:.1f}% of spam vs {ham_urls['short_url_percentage']:.1f}% of ham use shortened URLs\")\n\nprint(f\"\\n‚úÖ LENGTH HYPOTHESIS:\")\nprint(f\"   Spam emails: {spam_length['avg_length']:.0f} chars vs Ham: {ham_length['avg_length']:.0f} chars\")\nprint(f\"   Short emails: {spam_length['short_email_pct']:.1f}% spam vs {ham_length['short_email_pct']:.1f}% ham\")\n\nprint(f\"\\nüöÄ FEATURE ENGINEERING RECOMMENDATIONS:\")\nprint(f\"   - Create punctuation intensity features (!, ?, caps ratio)\")\nprint(f\"   - Binary URL presence and shortened URL detection\")\nprint(f\"   - Character substitution pattern detection\")\nprint(f\"   - Email length and word count features\")\nprint(f\"   - Spam-specific word frequency features\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T00:22:05.038404Z","iopub.execute_input":"2025-08-03T00:22:05.039180Z","iopub.status.idle":"2025-08-03T00:22:37.382099Z","shell.execute_reply.started":"2025-08-03T00:22:05.039148Z","shell.execute_reply":"2025-08-03T00:22:37.381161Z"}},"outputs":[{"name":"stdout","text":"üîç SPAM DETECTION EDA: VALIDATING FEATURE HYPOTHESES\n============================================================\nDataset shape: (83448, 2)\nClass distribution:\nlabel\n1    43910\n0    39538\nName: count, dtype: int64\nSpam rate: 52.6%\n\nSpam emails: 35128\nHam emails: 31630\n\nüìù PUNCTUATION ANALYSIS - SPAM\n----------------------------------------\n  avg_exclamations: 0.540\n  max_exclamations: 88.000\n  excessive_exclamations: 1521.000\n  avg_caps_ratio: 0.000\n  high_caps_emails: 0.000\n\nüìù PUNCTUATION ANALYSIS - HAM\n----------------------------------------\n  avg_exclamations: 0.238\n  max_exclamations: 46.000\n  excessive_exclamations: 341.000\n  avg_caps_ratio: 0.000\n  high_caps_emails: 0.000\n\nüåê URL ANALYSIS - SPAM\n------------------------------\n  Emails with URLs: 0 (0.0%)\n  Emails with shortened URLs: 8 (0.0%)\n  Total URLs found: 0\n\nüåê URL ANALYSIS - HAM\n------------------------------\n  Emails with URLs: 0 (0.0%)\n  Emails with shortened URLs: 16 (0.1%)\n  Total URLs found: 0\n\nüîÄ SUBSTITUTION ANALYSIS - SPAM\n-----------------------------------\n  number_subs: 8585 emails (24.4%)\n  at_symbol: 2698 emails (7.7%)\n  dollar_sign: 3069 emails (8.7%)\n  mixed_case: 0 emails (0.0%)\n  Example obfuscated words: ['20089', '20093', '20108', '20187', '20189']\n\nüîÄ SUBSTITUTION ANALYSIS - HAM\n-----------------------------------\n  number_subs: 11003 emails (34.8%)\n  at_symbol: 4684 emails (14.8%)\n  dollar_sign: 1754 emails (5.5%)\n  mixed_case: 0 emails (0.0%)\n  Example obfuscated words: ['2000', '2001', '2001', '94305', '5015']\n\nüìä WORD FREQUENCY - SPAM\n------------------------------\n  Top 15 words:\n     1. escapenumber    (262,589 times)\n     2. the             (211,849 times)\n     3. escapelong      (151,349 times)\n     4. to              (145,206 times)\n     5. and             (133,078 times)\n     6. of              (119,718 times)\n     7. a               (99,378 times)\n     8. you             (89,575 times)\n     9. in              (74,359 times)\n    10. for             (61,390 times)\n    11. is              (57,697 times)\n    12. your            (56,995 times)\n    13. this            (47,978 times)\n    14. i               (44,499 times)\n    15. that            (40,462 times)\n\nüìä WORD FREQUENCY - HAM\n------------------------------\n  Top 15 words:\n     1. escapenumber    (635,515 times)\n     2. the             (361,361 times)\n     3. to              (235,638 times)\n     4. a               (151,051 times)\n     5. and             (150,163 times)\n     6. of              (148,562 times)\n     7. in              (121,071 times)\n     8. for             (90,451 times)\n     9. i               (89,512 times)\n    10. is              (87,433 times)\n    11. on              (77,261 times)\n    12. that            (70,719 times)\n    13. this            (59,794 times)\n    14. you             (59,116 times)\n    15. it              (54,227 times)\n\nüìè LENGTH ANALYSIS - SPAM\n-------------------------\n  Average length: 1243.8 characters\n  Median length: 702.0 characters\n  Average words: 208.0 words\n  Short emails (<50 chars): 469 (1.3%)\n  Long emails (>500 chars): 22989 (65.4%)\n\nüìè LENGTH ANALYSIS - HAM\n-------------------------\n  Average length: 2084.1 characters\n  Median length: 1133.0 characters\n  Average words: 358.3 words\n  Short emails (<50 chars): 235 (0.7%)\n  Long emails (>500 chars): 25014 (79.1%)\n\nüéØ HYPOTHESIS VALIDATION SUMMARY\n==================================================\n‚úÖ PUNCTUATION HYPOTHESIS:\n   Spam emails use 0.5x more exclamations than ham\n   4.3% of spam vs 1.1% of ham use excessive exclamations\n\n‚úÖ URL HYPOTHESIS:\n   0.0% of spam vs 0.0% of ham contain URLs\n   0.0% of spam vs 0.1% of ham use shortened URLs\n\n‚úÖ LENGTH HYPOTHESIS:\n   Spam emails: 1244 chars vs Ham: 2084 chars\n   Short emails: 1.3% spam vs 0.7% ham\n\nüöÄ FEATURE ENGINEERING RECOMMENDATIONS:\n   - Create punctuation intensity features (!, ?, caps ratio)\n   - Binary URL presence and shortened URL detection\n   - Character substitution pattern detection\n   - Email length and word count features\n   - Spam-specific word frequency features\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================\n# BAG OF WORDS SPAM CLASSIFIER \n# ============================\n\n\ndef preprocess_email(text, \n                    to_lowercase=True,\n                    remove_punctuation=True, \n                    replace_urls=True,\n                    replace_numbers=True,\n                    stemming=True):\n    \"\"\"Clean and preprocess email text\"\"\"\n    \n    text = str(text)\n    \n    # Convert to lowercase\n    if to_lowercase:\n        text = text.lower()\n    \n    # Replace URLs with \"URL\"\n    if replace_urls:\n        url_pattern = r'http[s]?://\\S+|www\\.\\S+|\\S+\\.com\\S*'\n        text = re.sub(url_pattern, ' URL ', text)\n    \n    # Replace numbers with \"NUMBER\" \n    if replace_numbers:\n        text = re.sub(r'\\d+', ' NUMBER ', text)\n    \n    # Remove punctuation\n    if remove_punctuation:\n        text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Basic stemming (remove common suffixes) - simple approach\n    if stemming:\n        text = re.sub(r'ing\\b', '', text)  # running -> runn\n        text = re.sub(r'ed\\b', '', text)   # walked -> walk\n        text = re.sub(r'er\\b', '', text)   # better -> bett\n        text = re.sub(r'est\\b', '', text)  # fastest -> fast\n    \n    # Clean up extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef preprocess_emails(emails, **kwargs):\n    \"\"\"Preprocess a series of emails\"\"\"\n    return [preprocess_email(email, **kwargs) for email in emails]\n\ndef create_bow_features(X_train, X_test, \n                       binary=False,\n                       max_features=5000, \n                       ngram_range=(1,1),\n                       min_df=2):\n    \"\"\"Create Bag of Words features\"\"\"\n    \n    print(f\"üî§ Creating Bag of Words features...\")\n    print(f\"   Binary: {binary} | Max features: {max_features} | N-grams: {ngram_range}\")\n    \n    vectorizer = CountVectorizer(\n        binary=binary,                    # True = presence/absence, False = counts\n        max_features=max_features,        # Top N most frequent words\n        ngram_range=ngram_range,          # (1,1) = unigrams, (1,2) = unigrams + bigrams  \n        min_df=min_df,                    # Ignore rare words\n        max_df=0.95,                      # Ignore very common words\n        stop_words='english',             # Remove English stop words\n        token_pattern=r'\\b[a-zA-Z]{2,}\\b' # Only alphabetic words, min 2 chars\n    )\n    \n    # Fit on training, transform both\n    X_train_bow = vectorizer.fit_transform(X_train)\n    X_test_bow = vectorizer.transform(X_test)\n    \n    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n    print(f\"Training shape: {X_train_bow.shape}\")\n    print(f\"Sparsity: {1 - X_train_bow.nnz / (X_train_bow.shape[0] * X_train_bow.shape[1]):.3f}\")\n    \n    return X_train_bow, X_test_bow, vectorizer\n\ndef evaluate_models(X_train, y_train, y_test):\n    \"\"\"Evaluate multiple models on BoW features - FIXED VERSION\"\"\"\n    \n    print(f\"\\nü§ñ MODEL COMPARISON\")\n    print(\"=\" * 40)\n    \n    models = {\n        'LogisticRegression': LogisticRegression(random_state=42, max_iter=2000),\n        'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1),\n        'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n        'LightGBM': lgb.LGBMClassifier(random_state=42, verbosity=-1)\n    }\n    \n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    results = []\n    \n    for name, model in models.items():\n        print(f\"\\nüî• {name}...\")\n        start_time = time.time()\n        \n        # Handle LightGBM sparse matrix issue\n        if name == 'LightGBM':\n            X_train_use = X_train.toarray()  # Convert to dense\n        else:\n            X_train_use = X_train\n        \n        # Cross-validation (using processed data)\n        cv_scores = cross_val_score(model, X_train_use, y_train, cv=cv, scoring='roc_auc')\n        cv_f1 = cross_val_score(model, X_train_use, y_train, cv=cv, scoring='f1')\n        \n        training_time = time.time() - start_time\n        \n        results.append({\n            'Model': name,\n            'CV_AUC': f\"{cv_scores.mean():.4f} (¬±{cv_scores.std():.3f})\",\n            'CV_F1': f\"{cv_f1.mean():.4f}\",\n            'Time': f\"{training_time:.1f}s\",\n            'cv_auc_numeric': cv_scores.mean()  # For sorting by CV, not test\n        })\n        \n        print(f\"   CV AUC: {cv_scores.mean():.4f} (¬±{cv_scores.std():.3f})\")\n        print(f\"   CV F1: {cv_f1.mean():.4f}\")\n    \n    # Display results sorted by CV performance\n    results_df = pd.DataFrame(results).sort_values('cv_auc_numeric', ascending=False)\n    print(f\"\\nüèÜ RESULTS (sorted by CV AUC):\")\n    print(results_df.drop('cv_auc_numeric', axis=1).to_string(index=False))\n    \n    return results_df\n\n# Then separately, evaluate ONLY the best model on test set:\ndef final_test_evaluation(best_model_name, X_train, X_test, y_train, y_test):\n    \"\"\"Final evaluation on test set - use only once!\"\"\"\n    print(f\"\\nüéØ FINAL TEST EVALUATION: {best_model_name}\")\n    print(\"=\" * 40)\n    \n    # Recreate best model\n    models = {\n        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n        'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1),\n        'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0),\n        'LightGBM': lgb.LGBMClassifier(random_state=42, verbosity=-1)\n    }\n    \n    model = models[best_model_name]\n    \n    # Handle LightGBM\n    if best_model_name == 'LightGBM':\n        X_train_use = X_train.toarray()\n        X_test_use = X_test.toarray()\n    else:\n        X_train_use = X_train\n        X_test_use = X_test\n    \n    # Train and test\n    model.fit(X_train_use, y_train)\n    y_pred_proba = model.predict_proba(X_test_use)[:, 1]\n    test_auc = roc_auc_score(y_test, y_pred_proba)\n    \n    print(f\"Final Test AUC: {test_auc:.4f}\")\n    \n    return test_auc\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nprint(\"üöÄ BAG OF WORDS SPAM CLASSIFICATION\")\nprint(\"=\" * 50)\n\n\nprint(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n\n# ============================================================================\n# EXPERIMENT 1: Minimal preprocessing + Binary BoW\n# ============================================================================\n\nprint(f\"\\n\" + \"=\"*20 + \" EXPERIMENT 1: MINIMAL PREPROCESSING \" + \"=\"*20)\n\nX_train_clean1 = preprocess_emails(\n    X_train, \n    to_lowercase=True,\n    remove_punctuation=False,  # Keep punctuation \n    replace_urls=True,\n    replace_numbers=True,\n    stemming=False\n)\n\nX_test_clean1 = preprocess_emails(\n    X_test,\n    to_lowercase=True, \n    remove_punctuation=False,\n    replace_urls=True,\n    replace_numbers=True,\n    stemming=False\n)\n\n# Create BoW features\nX_train_bow1, X_test_bow1, vectorizer1 = create_bow_features(\n    X_train_clean1, X_test_clean1,\n    binary=True,              # Presence/absence\n    max_features=5000,\n    ngram_range=(1,1)         # Only unigrams\n)\n\n# Evaluate models\nresults1 = evaluate_models(X_train_bow1, y_train, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T00:22:37.383853Z","iopub.execute_input":"2025-08-03T00:22:37.384150Z","iopub.status.idle":"2025-08-03T00:32:24.566109Z","shell.execute_reply.started":"2025-08-03T00:22:37.384131Z","shell.execute_reply":"2025-08-03T00:32:24.565165Z"}},"outputs":[{"name":"stdout","text":"üöÄ BAG OF WORDS SPAM CLASSIFICATION\n==================================================\nTrain: 66758 | Test: 16690\n\n==================== EXPERIMENT 1: MINIMAL PREPROCESSING ====================\nüî§ Creating Bag of Words features...\n   Binary: True | Max features: 5000 | N-grams: (1, 1)\nVocabulary size: 5000\nTraining shape: (66758, 5000)\nSparsity: 0.987\n\nü§ñ MODEL COMPARISON\n========================================\n\nüî• LogisticRegression...\n   CV AUC: 0.9967 (¬±0.000)\n   CV F1: 0.9816\n\nüî• RandomForest...\n   CV AUC: 0.9974 (¬±0.000)\n   CV F1: 0.9815\n\nüî• XGBoost...\n   CV AUC: 0.9970 (¬±0.000)\n   CV F1: 0.9779\n\nüî• LightGBM...\n   CV AUC: 0.9971 (¬±0.000)\n   CV F1: 0.9788\n\nüèÜ RESULTS (sorted by CV AUC):\n             Model          CV_AUC  CV_F1   Time\n      RandomForest 0.9974 (¬±0.000) 0.9815 352.0s\n          LightGBM 0.9971 (¬±0.000) 0.9788 156.5s\n           XGBoost 0.9970 (¬±0.000) 0.9779  15.6s\nLogisticRegression 0.9967 (¬±0.000) 0.9816  26.7s\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# EXPERIMENT 2: Standard preprocessing + Count BoW\n# ============================================================================\n\nprint(f\"\\n\" + \"=\"*20 + \" EXPERIMENT 2: STANDARD PREPROCESSING \" + \"=\"*20)\n\nX_train_clean2 = preprocess_emails(\n    X_train,\n    to_lowercase=True,\n    remove_punctuation=True,   # Remove punctuation\n    replace_urls=True, \n    replace_numbers=True,\n    stemming=False\n)\n\nX_test_clean2 = preprocess_emails(\n    X_test,\n    to_lowercase=True,\n    remove_punctuation=True,\n    replace_urls=True,\n    replace_numbers=True, \n    stemming=False\n)\n\n# Create BoW features\nX_train_bow2, X_test_bow2, vectorizer2 = create_bow_features(\n    X_train_clean2, X_test_clean2,\n    binary=False,             # Word counts\n    max_features=5000,\n    ngram_range=(1,1)\n)\n\n# Evaluate models\nresults2 = evaluate_models(X_train_bow2, y_train, y_test)\n\n# ============================================================================\n# EXPERIMENT 3: Heavy preprocessing + Binary BoW + Bigrams\n# ============================================================================\n\nprint(f\"\\n\" + \"=\"*20 + \" EXPERIMENT 3: HEAVY PREPROCESSING + BIGRAMS \" + \"=\"*20)\n\nX_train_clean3 = preprocess_emails(\n    X_train,\n    to_lowercase=True,\n    remove_punctuation=True,\n    replace_urls=True,\n    replace_numbers=True,\n    stemming=True             # Simple stemming\n)\n\nX_test_clean3 = preprocess_emails(\n    X_test,\n    to_lowercase=True,\n    remove_punctuation=True,\n    replace_urls=True,\n    replace_numbers=True,\n    stemming=True\n)\n\n# Create BoW features with bigrams\nX_train_bow3, X_test_bow3, vectorizer3 = create_bow_features(\n    X_train_clean3, X_test_clean3,\n    binary=True,              # Presence/absence\n    max_features=10000,       # More features for bigrams\n    ngram_range=(1,2)         # Unigrams + bigrams\n)\n\n# Evaluate models\nresults3 = evaluate_models(X_train_bow3, y_train, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T00:32:24.567075Z","iopub.execute_input":"2025-08-03T00:32:24.567391Z","iopub.status.idle":"2025-08-03T01:00:31.376756Z","shell.execute_reply.started":"2025-08-03T00:32:24.567360Z","shell.execute_reply":"2025-08-03T01:00:31.375263Z"}},"outputs":[{"name":"stdout","text":"\n==================== EXPERIMENT 2: STANDARD PREPROCESSING ====================\nüî§ Creating Bag of Words features...\n   Binary: False | Max features: 5000 | N-grams: (1, 1)\nVocabulary size: 5000\nTraining shape: (66758, 5000)\nSparsity: 0.987\n\nü§ñ MODEL COMPARISON\n========================================\n\nüî• LogisticRegression...\n   CV AUC: 0.9952 (¬±0.001)\n   CV F1: 0.9813\n\nüî• RandomForest...\n   CV AUC: 0.9973 (¬±0.000)\n   CV F1: 0.9819\n\nüî• XGBoost...\n   CV AUC: 0.9971 (¬±0.000)\n   CV F1: 0.9788\n\nüî• LightGBM...\n   CV AUC: 0.9972 (¬±0.000)\n   CV F1: 0.9794\n\nüèÜ RESULTS (sorted by CV AUC):\n             Model          CV_AUC  CV_F1   Time\n      RandomForest 0.9973 (¬±0.000) 0.9819 344.7s\n          LightGBM 0.9972 (¬±0.000) 0.9794 173.1s\n           XGBoost 0.9971 (¬±0.000) 0.9788  38.6s\nLogisticRegression 0.9952 (¬±0.001) 0.9813 288.0s\n\n==================== EXPERIMENT 3: HEAVY PREPROCESSING + BIGRAMS ====================\nüî§ Creating Bag of Words features...\n   Binary: True | Max features: 10000 | N-grams: (1, 2)\nVocabulary size: 10000\nTraining shape: (66758, 10000)\nSparsity: 0.991\n\nü§ñ MODEL COMPARISON\n========================================\n\nüî• LogisticRegression...\n   CV AUC: 0.9974 (¬±0.000)\n   CV F1: 0.9845\n\nüî• RandomForest...\n   CV AUC: 0.9977 (¬±0.000)\n   CV F1: 0.9841\n\nüî• XGBoost...\n   CV AUC: 0.9972 (¬±0.000)\n   CV F1: 0.9785\n\nüî• LightGBM...\n   CV AUC: 0.9973 (¬±0.000)\n   CV F1: 0.9796\n\nüèÜ RESULTS (sorted by CV AUC):\n             Model          CV_AUC  CV_F1   Time\n      RandomForest 0.9977 (¬±0.000) 0.9841 361.7s\nLogisticRegression 0.9974 (¬±0.000) 0.9845  39.5s\n          LightGBM 0.9973 (¬±0.000) 0.9796 310.3s\n           XGBoost 0.9972 (¬±0.000) 0.9785  24.1s\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# SUMMARY (CORRECTED VERSION)\n# ============================================================================\nprint(f\"\\nüèÜ EXPERIMENT SUMMARY\")\nprint(\"=\" * 60)\n\n# Find best result from each experiment\nbest1 = results1.iloc[0]\nbest2 = results2.iloc[0] \nbest3 = results3.iloc[0]\n\n# Use CV_AUC instead of Test_AUC (which doesn't exist anymore)\nprint(f\"Experiment 1 (Minimal):     {best1['Model']} - CV AUC: {best1['CV_AUC']}\")\nprint(f\"Experiment 2 (Standard):    {best2['Model']} - CV AUC: {best2['CV_AUC']}\")\nprint(f\"Experiment 3 (Heavy+Bi):    {best3['Model']} - CV AUC: {best3['CV_AUC']}\")\n\n# Overall best using cv_auc_numeric instead of test_auc_numeric\nall_results = [\n    (1, best1['Model'], best1['cv_auc_numeric']),\n    (2, best2['Model'], best2['cv_auc_numeric']),\n    (3, best3['Model'], best3['cv_auc_numeric'])\n]\n\noverall_best = max(all_results, key=lambda x: x[2])\nprint(f\"\\nü•á OVERALL BEST: Experiment {overall_best[0]} - {overall_best[1]} (CV AUC: {overall_best[2]:.4f})\")\n\nprint(f\"\\n‚úÖ BAG OF WORDS SPAM CLASSIFICATION COMPLETE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T01:00:31.378849Z","iopub.execute_input":"2025-08-03T01:00:31.379206Z","iopub.status.idle":"2025-08-03T01:00:31.388702Z","shell.execute_reply.started":"2025-08-03T01:00:31.379179Z","shell.execute_reply":"2025-08-03T01:00:31.387612Z"}},"outputs":[{"name":"stdout","text":"\nüèÜ EXPERIMENT SUMMARY\n============================================================\nExperiment 1 (Minimal):     RandomForest - CV AUC: 0.9974 (¬±0.000)\nExperiment 2 (Standard):    RandomForest - CV AUC: 0.9973 (¬±0.000)\nExperiment 3 (Heavy+Bi):    RandomForest - CV AUC: 0.9977 (¬±0.000)\n\nü•á OVERALL BEST: Experiment 3 - RandomForest (CV AUC: 0.9977)\n\n‚úÖ BAG OF WORDS SPAM CLASSIFICATION COMPLETE!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"final_auc = final_test_evaluation('RandomForest', X_train_bow3, X_test_bow3, y_train, y_test)\nprint(f\"Final untouched test set performance: {final_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T01:00:31.389945Z","iopub.execute_input":"2025-08-03T01:00:31.390340Z","iopub.status.idle":"2025-08-03T01:01:22.922085Z","shell.execute_reply.started":"2025-08-03T01:00:31.390296Z","shell.execute_reply":"2025-08-03T01:01:22.921221Z"}},"outputs":[{"name":"stdout","text":"\nüéØ FINAL TEST EVALUATION: RandomForest\n========================================\nFinal Test AUC: 0.9979\nFinal untouched test set performance: 0.9979\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Retraining the best model\n\nThe winner on all experiments is the RandomForest model. The heavy preprocessing version beats the others by a fraction, so we'll go with that one. We will train the model one more time, but here's the crucial detail: **we will test this model on a completely new dataset because thre performance seems too good to be true**.\n\nLet's be real here. Spam comes in different shapes and sizes. Ham too, actually. I think our model has just memorized things very very well. You can say I've learnt a lot from my failed Titanic experiments!! Let's see practically how this model does on a different dataset, and then we'll see how good it truly is!\n\n*The new dataset to be used is one that somebody compiled and published on Kaggle after reading the same chapter that I'm reading right now, perfect!!!*","metadata":{}},{"cell_type":"code","source":"# ====================\n# TRAIN CHAMPION MODEL\n# ====================\n\n\nprint(\"üèÜ TRAINING FINAL CHAMPION MODEL: RandomForest (Experiment 3)\")\nprint(\"=\" * 60)\n\n# Recreate the winning configuration (Experiment 3)\nprint(\"\\nüìã Configuration:\")\nprint(\"   - Heavy preprocessing (lowercase, remove punctuation, URLs‚ÜíNUMBER, numbers‚ÜíNUMBER, stemming)\")\nprint(\"   - Binary BoW (presence/absence)\")\nprint(\"   - Bigrams (1-2 grams)\")\nprint(\"   - 10,000 max features\")\n\n# Train the final model\nprint(\"\\nüî• Training RandomForest on full training set...\")\nchampion_model = RandomForestClassifier(\n    random_state=42, \n    n_estimators=100,\n    n_jobs=-1\n)\n\n# Use Experiment 3 data (X_train_bow3, X_test_bow3)\nprint(f\"Training data shape: {X_train_bow3.shape}\")\n\n# Convert sparse to dense for RandomForest\nX_train_dense = X_train_bow3.toarray()\nchampion_model.fit(X_train_dense, y_train)\n\n# Quick verification on our test set\nX_test_dense = X_test_bow3.toarray()\ntest_predictions = champion_model.predict_proba(X_test_dense)[:, 1]\nverification_auc = roc_auc_score(y_test, test_predictions)\nprint(f\"\\nüìä Verification AUC on original test set: {verification_auc:.4f}\")\n\n# 3. The preprocessing configuration\npreprocessing_config = {\n    'to_lowercase': True,\n    'remove_punctuation': True,\n    'replace_urls': True,\n    'replace_numbers': True,\n    'stemming': True  \n}\n\n\nprint(f\"\\nüéØ Champion model ready for cross-dataset testing!\")\nprint(f\"   Training performance: 99.77% CV AUC\")\nprint(f\"   Test performance: {verification_auc:.4f} AUC\")\nprint(f\"\\nüöÄ Ready to test on new spam datasets!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T01:08:53.542610Z","iopub.execute_input":"2025-08-03T01:08:53.543019Z","iopub.status.idle":"2025-08-03T01:11:45.082826Z","shell.execute_reply.started":"2025-08-03T01:08:53.542992Z","shell.execute_reply":"2025-08-03T01:11:45.081777Z"}},"outputs":[{"name":"stdout","text":"üèÜ TRAINING FINAL CHAMPION MODEL: RandomForest (Experiment 3)\n============================================================\n\nüìã Configuration:\n   - Heavy preprocessing (lowercase, remove punctuation, URLs‚ÜíNUMBER, numbers‚ÜíNUMBER, stemming)\n   - Binary BoW (presence/absence)\n   - Bigrams (1-2 grams)\n   - 10,000 max features\n\nüî• Training RandomForest on full training set...\nTraining data shape: (66758, 10000)\n\nüìä Verification AUC on original test set: 0.9979\n\nüéØ Champion model ready for cross-dataset testing!\n   Training performance: 99.77% CV AUC\n   Test performance: 0.9979 AUC\n\nüöÄ Ready to test on new spam datasets!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"ROOT_DIR = Path(\"/kaggle/input/spam-or-not-spam-dataset\")\nDATA_DIR = ROOT_DIR / \"spam_or_not_spam.csv\"\nnew_df = pd.read_csv(DATA_DIR)\nX, y_new = new_df['email'], new_df['label']\nnew_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T01:43:52.136747Z","iopub.execute_input":"2025-08-03T01:43:52.137122Z","iopub.status.idle":"2025-08-03T01:43:52.212617Z","shell.execute_reply.started":"2025-08-03T01:43:52.137088Z","shell.execute_reply":"2025-08-03T01:43:52.211646Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                               email  label\n0   date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0\n1  martin a posted tassos papadopoulos the greek ...      0\n2  man threatens explosion in moscow thursday aug...      0\n3  klez the virus that won t die already the most...      0\n4   in adding cream to spaghetti carbonara which ...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>martin a posted tassos papadopoulos the greek ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>man threatens explosion in moscow thursday aug...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>klez the virus that won t die already the most...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>in adding cream to spaghetti carbonara which ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Assume you have:\n# 1. new_dataset_df: The new pandas DataFrame to evaluate.\n# 2. preprocess_clean_function: The same function used for original cleaning.\n# 3. vectorizer3: The vectorizer FITTED on the original X_train_clean3.\n# 4. champion_model: Your best trained model.\n# 5. y_new: The true labels from your new dataset.\n\n# Step 1: Apply the IDENTICAL preprocessing to the new text data.\n# Do not split it. You are evaluating the entire dataset.\nprint(\"‚û°Ô∏è Preprocessing new data...\")\nX_new_clean = preprocess_emails(\n    new_df['email'],\n    to_lowercase=True,\n    remove_punctuation=True,\n    replace_urls=True,\n    replace_numbers=True,\n    stemming=True\n)\n\n# Step 2: Use the ORIGINAL vectorizer to transform the new data. DO NOT RE-FIT.\nprint(\"‚û°Ô∏è Transforming new data into the model's feature space...\")\nX_new_bow = vectorizer3.transform(X_new_clean)\nX_new_dense = X_new_bow.toarray()\n\n# Step 3: Make predictions with your champion model.\nprint(\"‚û°Ô∏è Making predictions...\")\nnew_predictions = champion_model.predict_proba(X_new_dense)[:, 1]\n\n# Step 4: Evaluate performance.\nnew_auc = roc_auc_score(y_new, new_predictions)\n\nprint(f\"\\n‚úÖ Generalization AUC on new dataset: {new_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T01:43:53.269612Z","iopub.execute_input":"2025-08-03T01:43:53.269899Z","iopub.status.idle":"2025-08-03T01:43:55.188764Z","shell.execute_reply.started":"2025-08-03T01:43:53.269880Z","shell.execute_reply":"2025-08-03T01:43:55.187957Z"}},"outputs":[{"name":"stdout","text":"‚û°Ô∏è Preprocessing new data...\n‚û°Ô∏è Transforming new data into the model's feature space...\n‚û°Ô∏è Making predictions...\n\n‚úÖ Generalization AUC on new dataset: 0.9460\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### A little more confidence finally\n\n\nNot bad. A 0.4-0.5 drop in AUC score isn't bad at all considering we're still in the mid-90s of AUC scores on an entirely new dataset. To me, it seems impossible to be confident in any model I write unless it is tested on completely new datasets drawn from completely new sources, devoid of any connection. Only after doing so, I will be confident of any \"high scores\" I see...","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}