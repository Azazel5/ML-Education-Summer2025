Deep Dive into LLMs Like ChatGPT - https://www.youtube.com/watch?v=7xTGNNLPyMI
------------------------------------------------------------------------------


Introduction
------------

- How can you build something like this? Step 1 = pretraining. Download and process the internet. 
HuggingFace, https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1

- All LLM providers have some version of the FineWeb implemented internally. We filter the data agressively and 
make it only text so it ends up being about 44TB. CommonCrawl has been scouring the internet since 2007, and this
forms a core component of this. We eliminate lots of bad and dangerous websites right here, then we get the text
of the pages themselves. Since the internet is part of a global society of humans, we only keep English content
because these models have been created in America. Theoretically, one could create a version of this for any other
non-English speaking country too. What fraction of other languages do we want to see?

- We're trying to model how the text flows, the text of the internet, once processed and the dataset is finally
curated

- How are we going to represent text for the NNs? "QTFA encode" the text. We're talking about the vocabulary right
here, we don't want it to be too large. It is a finite, one-dimensional list of 0's and 1's that correspond to
characters that you and I know

- A naive way is: we take a group of these bits, like 8. Each byte will be a unique identifier, an easy way to
think about this is: each byte is a unique emoji. This is a precious resource, we want to keep shrinking this in
return for more symbols in the vocab. "Byte pair encoding" algorithm. GPT-4 uses 100277 symbols, and AK says this
is relatively a good size for SOTA models. We're converting to tokens and this is exactly what's called tokenization

- For tokenization, case and spacing all matters

Training the NN
---------------

- While training the NN, we take windows of the tokens we have formed. There are around 15 million tokens


Note - The video clarifies the difference between the number of symbols and the number of tokens. The number 100,277 
refers to the number of unique symbols (or the size of the vocabulary) that a model like GPT-4 uses. These symbols 
are created through a process called tokenization, using an algorithm like Byte Pair Encoding, which compresses text 
into a finite set of unique identifiers. The 15 trillion figure, however, refers to the total number of tokens in
the FineWeb dataset, which is the massive dataset of internet text used to train the language model. This means 
that while the model has a vocabulary of roughly 100,000 unique symbols to choose from, the total amount of text 
it was trained on consists of 15 trillion of these symbols in a long sequence. Think of it like this: a book might 
have 1,000 unique words (vocabulary size), but the book itself could contain 100,000 words in total (the length of 
the text). The terms "symbols" and "tokens" are used interchangeably in the video to represent these chunks of text

- Our NN takes in sequences of tokens, called the context length, which can be anywhere from 0 to 8000, although the
example picked in the video is 4. In principle, it can be infinite, but we constraint it because that's computationally
expensive

[https://excalidraw.com/ for tutorials ;)]

- https://bbycroft.net/llm

- The internal operations of transformers and LLMs are, while they may be important, AK definitely stresses that 
it is more important to learn the big picture, which is these LLMs are parametrized with, let's say, 85,000 or 
actually in the case of modern LLMs, probably millions, parameters. They go through some not so complex but many 
mathematical operations to give you the output

- The process will be, let's say we're doing inference, we feed our trained neural network or LLM one token, 
and then it is going to continuously, through a biased sampling, assign a higher probability for the more probable 
token being selected. It does this continuously. One thing to note is that these systems are stochastic in nature, 
so we are not going to get exact replicas of our training data, which is the entire internet. We get remixes. 
Sometimes there are randomized outputs that are unexpected from the perspectice of the training data

- GPT-2 published by OpenAI in 2019, https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
https://github.com/karpathy/llm.c/discussions/677
https://github.com/karpathy/llm.c/discussions/481

- Context length is different from context window

- How does AK actually run big LLM training? Like when he trained his own version of GPT-2? Cloud based. 8xH100 node.
Lambda labs. Kind of like Tufts HPC

- What companies release base models and what is it actually? It's an internet text token simulator. It isn't useful
yet, since we want an AI assistant! The base model is simple pre-training. 

1. GPT-2
2. LLaMA 3

https://arxiv.org/pdf/2407.21783

- Interacting with base models -> https://app.hyperbolic.ai/
The base model is not an assistant yet, it is just an expensive, glorified autocomplete
For the same prefix of tokens, we'll always get different answer. Stochasticity

The base models are queries in a Q/A way, where we pose the Q. Recall that things that occur frequently on the
internet can be trusted to a degree as compared to things that are rarer

- When AK did the Wikepedia experiment, the model regurgitates the information exactly... Some chance that there'll
be some things it will stray away from but sometimes it will give you copy-pasted items word for word

- It is often the case that when you have some documents that are really high quality, we sample from those
preferentially. Very fascinating experiment on testing models after their knowledge cutoff date. This is where the
model just hallucinates: takes its best, oftentimes wrong, guess
Base models can still be used, if you're clever with your prompt design. These models have in-context learning 
abilities through which we can take these models pretty far with few-shot examples

- Instantiate model assistant through prompting -> use ChatGPT or other LLMs to use prompts
Give the base models a conversation between an AI assistant and a human. It will go on and on which can be parsed
through clever structuring of the conversation. We can use the continuation as potential, "Next Questions" or just
discard them! 

- Post training is usually much less computationally expensive. But this is an important stage where we actually
turn these into an assistant. We feed the models multiple conversations. An entire large dataset for such 
conversations. Human labellers will be given some conversational contexts. This dataset is much smaller than the
pre-training dataset (the entire internet)

- Conversation from conversation to tokens, some sort of tokentization. TCP-IP packets as an analogy. Different
LLMs have different ways of doing this. GPT-40: im_start, user/assistant??, im_sep (internal monologue), question...
The model will start learning this encoding too, so it starts separating when the user writes and when it needs to
respond. So there is no programming here too, it is all MLAI

- During inference, the platform encodes all of this before sending it off to the model because we are only supplying
the questions. The queries

https://arxiv.org/pdf/2203.02155

- Open Assistant, HuggingFace.  Although OpenAI wrote that paper above, they did not fully explain the rationale and 
instructions which are according to AK around 300-400 pages long. But we have access to Open Assistant through which
we can actually see an example of such a dataset
There are many other examples too

- UltraChat (GitHub). Now we have LLMs which can help us create these datasets and conversations! Atlas Nomic to
visualize???

- When we talk to ChatGPT, we're talking to something that statistically imitates human labellers guided by the LLM
organization. These organizations hire experts; we're talking to average labeller simulations


Hallucinations
--------------

- Since the model statistically imitates the training set, even if deep inside its neurons or features it knows that
it doesn't know the answer to something, it will not say that. It is trained on answers not doubt. It will give a
confident answer because it has been trained on confident answers! 

- HuggingFace Inference Playground
If you ask the model to use tools or to not, they seem to follow instructions well

- So perhaps there are some examples we need to keep in our dataset where the model doesn't know the answer
How can we know when the model knows or doesn't? Empiracal probe. The Meta LLaMA way was to interrogate the model
and find its limits then work around there. Other LLMs can be used adversarially as judges to see how right the
answers are. We compare true answers to the generated answers. If in the case the model fails, we're going to add 
this question to the training set. And we'll make the answer, "I'm sorry, I don't know" or something like that.
Pretty smart

- Mitigation #2! Google search tool. Special tokens for the introduction of tools. The search text enters the
context window as a way to "freshen up" the model's memory about the topic and even serve as direct factual answers!
Once again, every new tool you introduce, you'll need to teach your model how to use it. Through few shot examples.
A few thousand examples, says the man. Funny discovery: due to the fact That these models have been trained on the 
entire internet, it actually has a good idea of what an internet search is. So it's able to pick it up pretty decently

- Vague recollection versus working memory. Knowledge in the parameters, "the neurons", is vague. Knowledge in the
context window is direct, manipulatable. Working memory. 
When possible, refresh the LLMs' memory! 


Knowledge of self 
-----------------

- It's a "token tumbler"
It has no self awareness. AK is right given my experiences with LLMs not even recognizing their own platforms and =
basically hallucinating stuff to give me an answer

- OLMO, the paper and everything is fully open source. SFT (Supervised Fine Tuning) mixture, the conversations 
dataset. The knowledge of self is usually finetuned, otherwise all models would answer, "I am ChatGPT by OpenAI", 
given that this is the most prominent model on the internet 

System messages is sometimes hidden in the platform, reminding the LLM of certain details


Models need tokens to think
---------------------------

- There's a finite amount of computation that happens for every token sequence input to the point where we can go
to output token probabilities. The more tokens we feed in, the more expensive the forward pass is. Not by much though.
Fixed amount of compute. We cannot expect any high level of computation out of any one token, so it is better to
distribute computations across many tokens. If you create prompts that answer the question immediately, you're training
the model to guess the answer in one single computation. This is why a CoT style answer is better

- We can also force the models to answer the question "in a single token" or something

- ChatGPT can use code. You can simply say, "Use code" in order to be crystal clear sure about math problems

- Models are not good for counting!! But it is good at copy pasting. Models aren't so good with spelling either

- The crazy part is that these models are solving such sophisticatedly difficult problems. But sometimes they 
fall short on simple problems such as which one of these two numbers is bigger 9.11 or 9.22. The fact that 
these models are trained on the entire internet's data can lead to unpredictable outcomes. 
One hypothesis is that the model thinks 9.11 or 9.22 that is reminiscent of Bible verses rather than floating point
numbers. And in such a context, 9.11 is larger than 9.22 I guess. Crazy


Reinforcement learning
----------------------

- Humans and LLMs have very different cognitions. While it is the case that the LLMs have incredibly vast amount of 
knowledge, first of all that knowledge is superficial. Second, the logical path by which some mathematical solutions
or spellings, those kind of things, are taken maybe non-obvious to LLMs. This is important to keep in mind

- So the process by which we train LLMS is divided into three categories. First was building the knowledge base, 
equivalent to reading all textbooks you are required to read when you're in school at once. Second was primed and 
tuning. This is where the language model becomes an AI assistant. This is where we feed our AI how to answer a 
question, a bunch of question answer series. So it learns how it's supposed to actually answer kind of like textbook
examples where the author shows this is how you're supposed to solve a problem and then comes the final stage which 
is the reinforcement learning stage. Kind of like the exercises at the end of each every chapter in math textbooks 
where the model tries varieties of situations in a playground setting and tries to get to the correct answer and 
keeps trying on and on until it does

- One cool thing about reinforcement learning is that the model starts to learn chain of thought reasoning kind of 
it finds aha moments where it experiments with multiple different ways of attacking a problem and then It 
backtracks whenever it finds a better way and then it goes back to the original one then backtracks again b
asically it goes back and forth back and forth kind of like how we Go back and forth whenever we're trying to 
solve a problem. It mimics our thinking style. This is a reasoning or "thinking" model

- Together.ai hosts DeepSeek R1. There are other SOTA models here too

- https://ics.uci.edu/~dechter/courses/ics-295/winter-2018/papers/nature-go.pdf

- 