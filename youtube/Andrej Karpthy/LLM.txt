How I use LLMS - https://www.youtube.com/watch?v=EWvNQjAaOHw
------------------------------------------------------------

Noteworthy LLMs = ChatGPT, Claude, Gemini, Deepseek, Grok, Le Chat, Perplexity, LLaMA, Copilot


Intro
-----

- Many LLMs out there, https://lmarena.ai/ a website which provides an interface to interact with many models and
rank them

- Seal leaderboard from Scale


Interacting with LLMs
---------------------

- These models are really good at writing! 

- Remember that every one of your queries are actually chopped into tokens. Use TikTokernizer to see how this works!

- Under the hood, a lot more is happening because we save a lot of conversational metadata as well. Think about it:
we have context windows and things like that. What looks like chat bubbles in the interface are actually 
token streams consisting of areas where my contributions began, ended, and where the LLM's prior contributions
began and ended too. A "New Chat" wipes the token window and resets everything! That is precisely the context window.

- These models are always a little bit out of date because the pre-training process, the important bit, isn't done
that often, as it is both time consuming and heavy on the $$ side. "Knowledge cutoff"
Post training is the bit that transforms the contents of the internet/next word predictor machine that an LLM is
to have "helpful chatbot vibes"

Post-training is where our dataset becomes conversations of humans 
Knowledge from pre-training, style and form from post-training

The pre-training knowledge is probabilistic and slightly vague

- What information is okay to ask something like ChatGPT? "How much caffeine in a shot of espresso?". This info
isn't recent and hasn't changed much. This information is frequent on the internet! There's no guarantee that this
is right though. We're talking about just the model, not the internet search tools available on model platforms
nowadays. This isn't a high stakes situation either. At the same time, skepticism is paramount, especially when it
comes to the details

- Anytime you start a new topic, always make new chats. Why? Tokens are expensive. As your window grows larger,
the model can get "distracted". Could decrease accuracy and performance. The more tokens in the window, the more
expensive to calculate the next token, slower. Precious resource. Keep it as short as you can

Pre-training -> Supervised finetuning -> Reinforcement Learning (thanks to Deepseek)

- Different providers have different categories of models: thinking models or not. Some models may be good even if
they are not thinking models. For difficult problems (eg. math and code), we'd want thinking models. Some generic
advice? No thinking model needed

- Tool usage: language model is just a 1TB or so zip file, a neural network with a trillion or so parameters. It
has no tools, and it can emit a string of tokens when fed your own query, transformed into a string of tokens itself

- An internet search will have a "search the internet" token which the platform will do. It will crawl all websites
related to the query, create tokens out of it, and feed it to our model. Without internet search, the model has
no chance to give you relively new information that is coming out AND is the type that is out there on the internet,
example, "when is the new season of white lotus coming out?"

- Sometimes these models can detect when the question references new knowledge and would perform a search by itself


Real-time searching
-------------------

- This is mandatory for reasons you know by now. But use Perplixity for initial search, just to see what's out
there. Whenever something is prone to change or changes, this is the methodology

- Deep research = internet search + thinking. Lots of thinking. All thinking models pretty much use RL now

- Think about it: when you upload files to LLMs, do they discard the images in them? AK thinks so. Or if the
images are there but they're difficult to understand. Or the images are converted to text document or something so
they can converted to a token stream

- It is rarely the case that AK reads papers, books without LLMs nowadays. Think about it: it is the absolute best
way to read. Also ask trivial questions about geography, quick fix health, etc/any question about your curiosities
Use Eooks here. Start with a summary and then ask follow up questions. It dramatically increases his retention,
understanding. Especially helpful for documents from other fields or past... 

- Opportunity to build exactly that: whatever AK says does not exist: book reader tool =============


Python
------

- Using agentic AI, giving access to a python interpreter
Special tokens for running code

- OpenAI has trained ChatGPT to recognize problems it cannot do "in its head" and to rely on tools instead
Keep track of which LLMs have access to what tools

Claude has an interpreter (JS), and ChatGPT does too it seems (Python), but Grok doesn't. Gemini doesn't seem
to have that as well, although 2.5 Pro seems to be better than Grok at math

- ChatGPT advanced data analyst. Pay attention to the code it writes too, it sneakily does things by itself
The tool is powerful but needs to be scrutized very well. Watch ChatGPT advanced data analysis videos ======


Artifacts (Claude)
------------------

- Claude artifcats are manipulable right on the platform! Use this... 
"Use artifacts featuture"

- You can use the example AK shows to practice for exams and learn material better. Not just for studies, for any
important thing we want to memorize. Limitless. claudeartifacts.com

- Diagram generation! "Conceptual diagram" "draw up a graph representing the relationships". Books, chapters, 
source code, and more. Your creativity is your only limiting factor! 


Misc
----

- Windsurf, cursor
- The composer is an autonomous agent in the codebase. It will execute commands, edit files, whatever. "Vibe coding"
- Average web dev is completely futile now it seems... 


Modalities
----------

- So far we have been interacting using text only
- Superwhisper*, WhisperFlow, MacWhisper, AK uses this to transcribe voice to text, basically the same thing you where
trying to do a couple weeks ago

- Remember that the model still only looks at text tokens. You break down audio into a spectogram to see the different
frequencies inside. Quantize them into tokens. You train the model with audio chunks so it can understand it!
The true audio button i.e. the black button on most models nowadays. No text involved here, pretty magical. You
can change the tonality and speech patterns immediately! It really can do a lot. Possesses a lot of knowledge...

Audio tokens back and forth not the thing from before which is voice->text which the model consumes. The voice one
refuses a lot, AK has noticed. Then move to text

- Grok has lots of modes and very less boundaries and restrictions

- NotebookLM. And also Google AI Studio. Podcast generator! To listen to and also to publish!!
Useful on walks and drives and other niche cases. Passive interests

- Images can be represented in token streams too. Images converted to rectangular quantized patches 

- The transformer neural network used by LLMs don't even know that some input may be audio, some may be images, or
what. At the decoding phase, we will get back whatever we asked for essentially
Upload nutrition labels! Ask questions... First make it transcribe to text, make sure that the model reads the
values correctly, then ask your questions

- AK also asks questions about his blood tests
Blood test results are not esoteric. So, once more, we know how to determine if a question is LLM worthy or not,
especially in the case of high stakes things like our own health. OpenAI DALLE for image output. Not perfect at all,
but not bad! Thumbnail generation! Ideaogram.ai is a competitor for DALLE. Icons

This isn't done fully in the model. DALLE is a separate model that takes text and creates images

- Video input. Included in advanced voice, as you've seen. You can just open the camera and solve problems
immediately! 
AK thinks even in this mode, the model actually takes image chunks and doesn't actually process the video

Video output = Sora + many, many others (Vo2 seems SOTA)

Quality of life features
------------------------

- ChatGPT memory feature. There is an ability to save information from chat to chat, which is cool because we
know that a new chat typically means context window is wiped clean. It has to be invoked! Ask it to remember
stuff, then reference it in another chat -> ChatGPT -> "Memory updated" / Manage memories. This is editable

- Custom instructions. You can modify these LLMs. Present in ChatGPT for sure, but other LLMs are tweakable too

- Custom GPTs, used for language learning! Use LLMs to learn Italian. Extract vocab in dictionary form. Give
concrete examples and instructions. Few-shot, CoT. If there's a certain prompt you keep reusing, make a custom
GPT, use it. Like a function you write in code

- AK goes as far as to say, don't use any other translator. Create a custom GPT, it will be more accurate and you
can ask it clarifying questions. Learn more than just the words, learn sentence structure, slang, nuance, deeply